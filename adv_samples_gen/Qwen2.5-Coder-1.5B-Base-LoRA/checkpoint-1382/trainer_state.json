{
  "best_metric": 1.365147352218628,
  "best_model_checkpoint": "Qwen2.5-Coder-1.5B-Base-LoRA/checkpoint-1382",
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1382,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01447178002894356,
      "grad_norm": 0.36929914355278015,
      "learning_rate": 2.403846153846154e-06,
      "loss": 1.9208,
      "step": 10
    },
    {
      "epoch": 0.02894356005788712,
      "grad_norm": 0.39211782813072205,
      "learning_rate": 4.807692307692308e-06,
      "loss": 1.8204,
      "step": 20
    },
    {
      "epoch": 0.04341534008683068,
      "grad_norm": 0.4012216329574585,
      "learning_rate": 7.211538461538461e-06,
      "loss": 1.9032,
      "step": 30
    },
    {
      "epoch": 0.05788712011577424,
      "grad_norm": 0.4030298590660095,
      "learning_rate": 9.615384615384616e-06,
      "loss": 1.8581,
      "step": 40
    },
    {
      "epoch": 0.0723589001447178,
      "grad_norm": 0.4179970622062683,
      "learning_rate": 1.2019230769230771e-05,
      "loss": 1.7513,
      "step": 50
    },
    {
      "epoch": 0.08683068017366136,
      "grad_norm": 0.5003582239151001,
      "learning_rate": 1.4423076923076923e-05,
      "loss": 1.7814,
      "step": 60
    },
    {
      "epoch": 0.10130246020260492,
      "grad_norm": 0.5727551579475403,
      "learning_rate": 1.682692307692308e-05,
      "loss": 1.8555,
      "step": 70
    },
    {
      "epoch": 0.11577424023154848,
      "grad_norm": 0.5393267273902893,
      "learning_rate": 1.923076923076923e-05,
      "loss": 1.6515,
      "step": 80
    },
    {
      "epoch": 0.13024602026049203,
      "grad_norm": 0.9663665890693665,
      "learning_rate": 2.1634615384615387e-05,
      "loss": 1.7988,
      "step": 90
    },
    {
      "epoch": 0.1447178002894356,
      "grad_norm": 0.9453794956207275,
      "learning_rate": 2.4038461538461542e-05,
      "loss": 1.6899,
      "step": 100
    },
    {
      "epoch": 0.15918958031837915,
      "grad_norm": 1.140159249305725,
      "learning_rate": 2.6442307692307694e-05,
      "loss": 1.5665,
      "step": 110
    },
    {
      "epoch": 0.1736613603473227,
      "grad_norm": 1.6329216957092285,
      "learning_rate": 2.8846153846153845e-05,
      "loss": 1.4996,
      "step": 120
    },
    {
      "epoch": 0.18813314037626627,
      "grad_norm": 1.2995994091033936,
      "learning_rate": 3.125e-05,
      "loss": 1.4374,
      "step": 130
    },
    {
      "epoch": 0.20260492040520983,
      "grad_norm": 1.8936522006988525,
      "learning_rate": 3.365384615384616e-05,
      "loss": 1.2524,
      "step": 140
    },
    {
      "epoch": 0.2170767004341534,
      "grad_norm": 2.6967902183532715,
      "learning_rate": 3.605769230769231e-05,
      "loss": 1.1226,
      "step": 150
    },
    {
      "epoch": 0.23154848046309695,
      "grad_norm": 2.16520619392395,
      "learning_rate": 3.846153846153846e-05,
      "loss": 0.9251,
      "step": 160
    },
    {
      "epoch": 0.2460202604920405,
      "grad_norm": 3.2051076889038086,
      "learning_rate": 4.0865384615384615e-05,
      "loss": 0.83,
      "step": 170
    },
    {
      "epoch": 0.26049204052098407,
      "grad_norm": 1.9492558240890503,
      "learning_rate": 4.326923076923077e-05,
      "loss": 0.7526,
      "step": 180
    },
    {
      "epoch": 0.27496382054992763,
      "grad_norm": 2.7950568199157715,
      "learning_rate": 4.5673076923076925e-05,
      "loss": 0.6633,
      "step": 190
    },
    {
      "epoch": 0.2894356005788712,
      "grad_norm": 2.5034396648406982,
      "learning_rate": 4.8076923076923084e-05,
      "loss": 0.6206,
      "step": 200
    },
    {
      "epoch": 0.30390738060781475,
      "grad_norm": 2.9437172412872314,
      "learning_rate": 4.994638069705094e-05,
      "loss": 0.521,
      "step": 210
    },
    {
      "epoch": 0.3183791606367583,
      "grad_norm": 2.735271692276001,
      "learning_rate": 4.967828418230563e-05,
      "loss": 0.5617,
      "step": 220
    },
    {
      "epoch": 0.33285094066570187,
      "grad_norm": 3.705606460571289,
      "learning_rate": 4.941018766756033e-05,
      "loss": 0.4491,
      "step": 230
    },
    {
      "epoch": 0.3473227206946454,
      "grad_norm": 3.066352367401123,
      "learning_rate": 4.914209115281502e-05,
      "loss": 0.48,
      "step": 240
    },
    {
      "epoch": 0.361794500723589,
      "grad_norm": 4.3537068367004395,
      "learning_rate": 4.8873994638069706e-05,
      "loss": 0.4516,
      "step": 250
    },
    {
      "epoch": 0.37626628075253254,
      "grad_norm": 5.612321853637695,
      "learning_rate": 4.86058981233244e-05,
      "loss": 0.3942,
      "step": 260
    },
    {
      "epoch": 0.3907380607814761,
      "grad_norm": 2.4387030601501465,
      "learning_rate": 4.833780160857909e-05,
      "loss": 0.4057,
      "step": 270
    },
    {
      "epoch": 0.40520984081041966,
      "grad_norm": 3.6199734210968018,
      "learning_rate": 4.806970509383379e-05,
      "loss": 0.4252,
      "step": 280
    },
    {
      "epoch": 0.4196816208393632,
      "grad_norm": 2.287996768951416,
      "learning_rate": 4.7801608579088476e-05,
      "loss": 0.4122,
      "step": 290
    },
    {
      "epoch": 0.4341534008683068,
      "grad_norm": 2.003175973892212,
      "learning_rate": 4.7533512064343165e-05,
      "loss": 0.4445,
      "step": 300
    },
    {
      "epoch": 0.44862518089725034,
      "grad_norm": 1.633800983428955,
      "learning_rate": 4.726541554959786e-05,
      "loss": 0.3887,
      "step": 310
    },
    {
      "epoch": 0.4630969609261939,
      "grad_norm": 1.325947880744934,
      "learning_rate": 4.699731903485255e-05,
      "loss": 0.4155,
      "step": 320
    },
    {
      "epoch": 0.47756874095513746,
      "grad_norm": 2.4757699966430664,
      "learning_rate": 4.672922252010724e-05,
      "loss": 0.3748,
      "step": 330
    },
    {
      "epoch": 0.492040520984081,
      "grad_norm": 2.197216033935547,
      "learning_rate": 4.6461126005361935e-05,
      "loss": 0.3965,
      "step": 340
    },
    {
      "epoch": 0.5065123010130246,
      "grad_norm": 2.5536770820617676,
      "learning_rate": 4.6193029490616624e-05,
      "loss": 0.3796,
      "step": 350
    },
    {
      "epoch": 0.5209840810419681,
      "grad_norm": 2.9924068450927734,
      "learning_rate": 4.592493297587131e-05,
      "loss": 0.3439,
      "step": 360
    },
    {
      "epoch": 0.5354558610709117,
      "grad_norm": 2.1704630851745605,
      "learning_rate": 4.565683646112601e-05,
      "loss": 0.3984,
      "step": 370
    },
    {
      "epoch": 0.5499276410998553,
      "grad_norm": 1.5833989381790161,
      "learning_rate": 4.53887399463807e-05,
      "loss": 0.4111,
      "step": 380
    },
    {
      "epoch": 0.5643994211287988,
      "grad_norm": 1.601758360862732,
      "learning_rate": 4.512064343163539e-05,
      "loss": 0.3558,
      "step": 390
    },
    {
      "epoch": 0.5788712011577424,
      "grad_norm": 1.8361541032791138,
      "learning_rate": 4.485254691689008e-05,
      "loss": 0.3701,
      "step": 400
    },
    {
      "epoch": 0.5933429811866859,
      "grad_norm": 2.0945613384246826,
      "learning_rate": 4.458445040214477e-05,
      "loss": 0.3324,
      "step": 410
    },
    {
      "epoch": 0.6078147612156295,
      "grad_norm": 2.3101398944854736,
      "learning_rate": 4.431635388739947e-05,
      "loss": 0.3609,
      "step": 420
    },
    {
      "epoch": 0.622286541244573,
      "grad_norm": 1.8041008710861206,
      "learning_rate": 4.4048257372654156e-05,
      "loss": 0.3389,
      "step": 430
    },
    {
      "epoch": 0.6367583212735166,
      "grad_norm": 1.9689791202545166,
      "learning_rate": 4.3780160857908846e-05,
      "loss": 0.3276,
      "step": 440
    },
    {
      "epoch": 0.6512301013024602,
      "grad_norm": 1.6356033086776733,
      "learning_rate": 4.351206434316354e-05,
      "loss": 0.3316,
      "step": 450
    },
    {
      "epoch": 0.6657018813314037,
      "grad_norm": 1.1536304950714111,
      "learning_rate": 4.324396782841823e-05,
      "loss": 0.3658,
      "step": 460
    },
    {
      "epoch": 0.6801736613603473,
      "grad_norm": 1.5488700866699219,
      "learning_rate": 4.297587131367292e-05,
      "loss": 0.3542,
      "step": 470
    },
    {
      "epoch": 0.6946454413892909,
      "grad_norm": 1.5805628299713135,
      "learning_rate": 4.2707774798927615e-05,
      "loss": 0.322,
      "step": 480
    },
    {
      "epoch": 0.7091172214182344,
      "grad_norm": 2.2374658584594727,
      "learning_rate": 4.243967828418231e-05,
      "loss": 0.3327,
      "step": 490
    },
    {
      "epoch": 0.723589001447178,
      "grad_norm": 2.867985725402832,
      "learning_rate": 4.2171581769437e-05,
      "loss": 0.3655,
      "step": 500
    },
    {
      "epoch": 0.7380607814761215,
      "grad_norm": 2.1060307025909424,
      "learning_rate": 4.1903485254691696e-05,
      "loss": 0.3469,
      "step": 510
    },
    {
      "epoch": 0.7525325615050651,
      "grad_norm": 1.6457034349441528,
      "learning_rate": 4.1635388739946385e-05,
      "loss": 0.3478,
      "step": 520
    },
    {
      "epoch": 0.7670043415340086,
      "grad_norm": 1.9318182468414307,
      "learning_rate": 4.1367292225201074e-05,
      "loss": 0.3164,
      "step": 530
    },
    {
      "epoch": 0.7814761215629522,
      "grad_norm": 2.0623772144317627,
      "learning_rate": 4.109919571045577e-05,
      "loss": 0.3658,
      "step": 540
    },
    {
      "epoch": 0.7959479015918958,
      "grad_norm": 2.1293375492095947,
      "learning_rate": 4.083109919571046e-05,
      "loss": 0.3383,
      "step": 550
    },
    {
      "epoch": 0.8104196816208393,
      "grad_norm": 1.5329786539077759,
      "learning_rate": 4.056300268096515e-05,
      "loss": 0.3165,
      "step": 560
    },
    {
      "epoch": 0.8248914616497829,
      "grad_norm": 1.1330876350402832,
      "learning_rate": 4.0294906166219844e-05,
      "loss": 0.3351,
      "step": 570
    },
    {
      "epoch": 0.8393632416787264,
      "grad_norm": 1.725695252418518,
      "learning_rate": 4.002680965147453e-05,
      "loss": 0.3297,
      "step": 580
    },
    {
      "epoch": 0.85383502170767,
      "grad_norm": 2.366680860519409,
      "learning_rate": 3.975871313672922e-05,
      "loss": 0.3339,
      "step": 590
    },
    {
      "epoch": 0.8683068017366136,
      "grad_norm": 1.765604853630066,
      "learning_rate": 3.949061662198392e-05,
      "loss": 0.3108,
      "step": 600
    },
    {
      "epoch": 0.8827785817655571,
      "grad_norm": 1.3392983675003052,
      "learning_rate": 3.922252010723861e-05,
      "loss": 0.3519,
      "step": 610
    },
    {
      "epoch": 0.8972503617945007,
      "grad_norm": 0.9919654130935669,
      "learning_rate": 3.89544235924933e-05,
      "loss": 0.3122,
      "step": 620
    },
    {
      "epoch": 0.9117221418234442,
      "grad_norm": 1.8317092657089233,
      "learning_rate": 3.868632707774799e-05,
      "loss": 0.3727,
      "step": 630
    },
    {
      "epoch": 0.9261939218523878,
      "grad_norm": 1.2486661672592163,
      "learning_rate": 3.841823056300268e-05,
      "loss": 0.3127,
      "step": 640
    },
    {
      "epoch": 0.9406657018813314,
      "grad_norm": 2.404240369796753,
      "learning_rate": 3.815013404825738e-05,
      "loss": 0.3291,
      "step": 650
    },
    {
      "epoch": 0.9551374819102749,
      "grad_norm": 2.6676294803619385,
      "learning_rate": 3.7882037533512066e-05,
      "loss": 0.3465,
      "step": 660
    },
    {
      "epoch": 0.9696092619392185,
      "grad_norm": 1.7618036270141602,
      "learning_rate": 3.7613941018766755e-05,
      "loss": 0.3268,
      "step": 670
    },
    {
      "epoch": 0.984081041968162,
      "grad_norm": 1.4061739444732666,
      "learning_rate": 3.734584450402145e-05,
      "loss": 0.3043,
      "step": 680
    },
    {
      "epoch": 0.9985528219971056,
      "grad_norm": 1.7527203559875488,
      "learning_rate": 3.707774798927614e-05,
      "loss": 0.2992,
      "step": 690
    },
    {
      "epoch": 1.0,
      "eval_loss": 1.4096287488937378,
      "eval_runtime": 85.928,
      "eval_samples_per_second": 8.053,
      "eval_steps_per_second": 4.027,
      "step": 691
    },
    {
      "epoch": 1.0130246020260492,
      "grad_norm": 1.9394503831863403,
      "learning_rate": 3.680965147453083e-05,
      "loss": 0.3128,
      "step": 700
    },
    {
      "epoch": 1.0274963820549927,
      "grad_norm": 3.3055193424224854,
      "learning_rate": 3.6541554959785525e-05,
      "loss": 0.3309,
      "step": 710
    },
    {
      "epoch": 1.0419681620839363,
      "grad_norm": 1.458728551864624,
      "learning_rate": 3.6273458445040214e-05,
      "loss": 0.3487,
      "step": 720
    },
    {
      "epoch": 1.0564399421128798,
      "grad_norm": 1.3527613878250122,
      "learning_rate": 3.600536193029491e-05,
      "loss": 0.3024,
      "step": 730
    },
    {
      "epoch": 1.0709117221418234,
      "grad_norm": 1.3259472846984863,
      "learning_rate": 3.57372654155496e-05,
      "loss": 0.319,
      "step": 740
    },
    {
      "epoch": 1.085383502170767,
      "grad_norm": 2.2983999252319336,
      "learning_rate": 3.546916890080429e-05,
      "loss": 0.3219,
      "step": 750
    },
    {
      "epoch": 1.0998552821997105,
      "grad_norm": 1.4035834074020386,
      "learning_rate": 3.5201072386058984e-05,
      "loss": 0.3638,
      "step": 760
    },
    {
      "epoch": 1.114327062228654,
      "grad_norm": 1.5753453969955444,
      "learning_rate": 3.493297587131367e-05,
      "loss": 0.3491,
      "step": 770
    },
    {
      "epoch": 1.1287988422575976,
      "grad_norm": 1.5391076803207397,
      "learning_rate": 3.466487935656836e-05,
      "loss": 0.3356,
      "step": 780
    },
    {
      "epoch": 1.1432706222865412,
      "grad_norm": 1.8106788396835327,
      "learning_rate": 3.439678284182306e-05,
      "loss": 0.3125,
      "step": 790
    },
    {
      "epoch": 1.1577424023154848,
      "grad_norm": 1.9768937826156616,
      "learning_rate": 3.412868632707775e-05,
      "loss": 0.3457,
      "step": 800
    },
    {
      "epoch": 1.1722141823444283,
      "grad_norm": 3.1538732051849365,
      "learning_rate": 3.386058981233244e-05,
      "loss": 0.3409,
      "step": 810
    },
    {
      "epoch": 1.1866859623733719,
      "grad_norm": 1.2860502004623413,
      "learning_rate": 3.359249329758714e-05,
      "loss": 0.3058,
      "step": 820
    },
    {
      "epoch": 1.2011577424023154,
      "grad_norm": 1.38240647315979,
      "learning_rate": 3.332439678284183e-05,
      "loss": 0.3181,
      "step": 830
    },
    {
      "epoch": 1.215629522431259,
      "grad_norm": 1.6576428413391113,
      "learning_rate": 3.3056300268096516e-05,
      "loss": 0.3233,
      "step": 840
    },
    {
      "epoch": 1.2301013024602026,
      "grad_norm": 3.4875242710113525,
      "learning_rate": 3.278820375335121e-05,
      "loss": 0.3005,
      "step": 850
    },
    {
      "epoch": 1.244573082489146,
      "grad_norm": 1.3165050745010376,
      "learning_rate": 3.25201072386059e-05,
      "loss": 0.3316,
      "step": 860
    },
    {
      "epoch": 1.2590448625180897,
      "grad_norm": 1.9086107015609741,
      "learning_rate": 3.225201072386059e-05,
      "loss": 0.3158,
      "step": 870
    },
    {
      "epoch": 1.2735166425470332,
      "grad_norm": 1.3196560144424438,
      "learning_rate": 3.1983914209115286e-05,
      "loss": 0.3184,
      "step": 880
    },
    {
      "epoch": 1.2879884225759768,
      "grad_norm": 1.6899358034133911,
      "learning_rate": 3.1715817694369975e-05,
      "loss": 0.3301,
      "step": 890
    },
    {
      "epoch": 1.3024602026049203,
      "grad_norm": 1.4773435592651367,
      "learning_rate": 3.1447721179624664e-05,
      "loss": 0.2982,
      "step": 900
    },
    {
      "epoch": 1.316931982633864,
      "grad_norm": 1.3645135164260864,
      "learning_rate": 3.117962466487936e-05,
      "loss": 0.2865,
      "step": 910
    },
    {
      "epoch": 1.3314037626628075,
      "grad_norm": 1.7528982162475586,
      "learning_rate": 3.091152815013405e-05,
      "loss": 0.3243,
      "step": 920
    },
    {
      "epoch": 1.345875542691751,
      "grad_norm": 1.283546805381775,
      "learning_rate": 3.0643431635388745e-05,
      "loss": 0.2752,
      "step": 930
    },
    {
      "epoch": 1.3603473227206946,
      "grad_norm": 1.957701563835144,
      "learning_rate": 3.0375335120643434e-05,
      "loss": 0.2812,
      "step": 940
    },
    {
      "epoch": 1.3748191027496381,
      "grad_norm": 2.720132827758789,
      "learning_rate": 3.0107238605898126e-05,
      "loss": 0.3093,
      "step": 950
    },
    {
      "epoch": 1.3892908827785817,
      "grad_norm": 1.4574260711669922,
      "learning_rate": 2.9839142091152816e-05,
      "loss": 0.2955,
      "step": 960
    },
    {
      "epoch": 1.4037626628075253,
      "grad_norm": 1.2827991247177124,
      "learning_rate": 2.9571045576407508e-05,
      "loss": 0.3116,
      "step": 970
    },
    {
      "epoch": 1.4182344428364688,
      "grad_norm": 1.6113032102584839,
      "learning_rate": 2.93029490616622e-05,
      "loss": 0.2996,
      "step": 980
    },
    {
      "epoch": 1.4327062228654124,
      "grad_norm": 1.9279792308807373,
      "learning_rate": 2.903485254691689e-05,
      "loss": 0.3051,
      "step": 990
    },
    {
      "epoch": 1.447178002894356,
      "grad_norm": 1.3089144229888916,
      "learning_rate": 2.8766756032171582e-05,
      "loss": 0.2927,
      "step": 1000
    },
    {
      "epoch": 1.4616497829232995,
      "grad_norm": 1.7725530862808228,
      "learning_rate": 2.8498659517426274e-05,
      "loss": 0.314,
      "step": 1010
    },
    {
      "epoch": 1.476121562952243,
      "grad_norm": 0.9603679776191711,
      "learning_rate": 2.8230563002680967e-05,
      "loss": 0.2923,
      "step": 1020
    },
    {
      "epoch": 1.4905933429811866,
      "grad_norm": 2.015976905822754,
      "learning_rate": 2.7962466487935656e-05,
      "loss": 0.2994,
      "step": 1030
    },
    {
      "epoch": 1.5050651230101302,
      "grad_norm": 1.991795539855957,
      "learning_rate": 2.769436997319035e-05,
      "loss": 0.2866,
      "step": 1040
    },
    {
      "epoch": 1.5195369030390737,
      "grad_norm": 2.241424798965454,
      "learning_rate": 2.742627345844504e-05,
      "loss": 0.2992,
      "step": 1050
    },
    {
      "epoch": 1.5340086830680173,
      "grad_norm": 1.1073205471038818,
      "learning_rate": 2.715817694369973e-05,
      "loss": 0.2984,
      "step": 1060
    },
    {
      "epoch": 1.5484804630969609,
      "grad_norm": 1.34968101978302,
      "learning_rate": 2.6890080428954422e-05,
      "loss": 0.2872,
      "step": 1070
    },
    {
      "epoch": 1.5629522431259044,
      "grad_norm": 1.6301177740097046,
      "learning_rate": 2.6621983914209115e-05,
      "loss": 0.2729,
      "step": 1080
    },
    {
      "epoch": 1.577424023154848,
      "grad_norm": 0.9653958082199097,
      "learning_rate": 2.6353887399463807e-05,
      "loss": 0.3092,
      "step": 1090
    },
    {
      "epoch": 1.5918958031837915,
      "grad_norm": 2.479123115539551,
      "learning_rate": 2.6085790884718496e-05,
      "loss": 0.3034,
      "step": 1100
    },
    {
      "epoch": 1.606367583212735,
      "grad_norm": 1.4833327531814575,
      "learning_rate": 2.5817694369973195e-05,
      "loss": 0.3005,
      "step": 1110
    },
    {
      "epoch": 1.6208393632416787,
      "grad_norm": 1.7180174589157104,
      "learning_rate": 2.5549597855227885e-05,
      "loss": 0.2718,
      "step": 1120
    },
    {
      "epoch": 1.6353111432706222,
      "grad_norm": 1.2988097667694092,
      "learning_rate": 2.5281501340482577e-05,
      "loss": 0.31,
      "step": 1130
    },
    {
      "epoch": 1.6497829232995658,
      "grad_norm": 2.980414628982544,
      "learning_rate": 2.501340482573727e-05,
      "loss": 0.3334,
      "step": 1140
    },
    {
      "epoch": 1.6642547033285093,
      "grad_norm": 1.4315245151519775,
      "learning_rate": 2.474530831099196e-05,
      "loss": 0.2647,
      "step": 1150
    },
    {
      "epoch": 1.678726483357453,
      "grad_norm": 2.0592823028564453,
      "learning_rate": 2.4477211796246648e-05,
      "loss": 0.2774,
      "step": 1160
    },
    {
      "epoch": 1.6931982633863965,
      "grad_norm": 1.2779614925384521,
      "learning_rate": 2.420911528150134e-05,
      "loss": 0.295,
      "step": 1170
    },
    {
      "epoch": 1.70767004341534,
      "grad_norm": 1.1130836009979248,
      "learning_rate": 2.3941018766756032e-05,
      "loss": 0.2849,
      "step": 1180
    },
    {
      "epoch": 1.7221418234442836,
      "grad_norm": 1.8558309078216553,
      "learning_rate": 2.3672922252010725e-05,
      "loss": 0.2934,
      "step": 1190
    },
    {
      "epoch": 1.7366136034732271,
      "grad_norm": 1.8622244596481323,
      "learning_rate": 2.3404825737265417e-05,
      "loss": 0.3016,
      "step": 1200
    },
    {
      "epoch": 1.7510853835021707,
      "grad_norm": 1.4693292379379272,
      "learning_rate": 2.313672922252011e-05,
      "loss": 0.2829,
      "step": 1210
    },
    {
      "epoch": 1.7655571635311142,
      "grad_norm": 1.2285068035125732,
      "learning_rate": 2.2868632707774802e-05,
      "loss": 0.2906,
      "step": 1220
    },
    {
      "epoch": 1.7800289435600578,
      "grad_norm": 1.7683045864105225,
      "learning_rate": 2.260053619302949e-05,
      "loss": 0.2787,
      "step": 1230
    },
    {
      "epoch": 1.7945007235890014,
      "grad_norm": 1.7110928297042847,
      "learning_rate": 2.2332439678284184e-05,
      "loss": 0.2801,
      "step": 1240
    },
    {
      "epoch": 1.808972503617945,
      "grad_norm": 1.3165299892425537,
      "learning_rate": 2.2064343163538876e-05,
      "loss": 0.3032,
      "step": 1250
    },
    {
      "epoch": 1.8234442836468885,
      "grad_norm": 2.4506027698516846,
      "learning_rate": 2.1796246648793565e-05,
      "loss": 0.2902,
      "step": 1260
    },
    {
      "epoch": 1.837916063675832,
      "grad_norm": 1.8064130544662476,
      "learning_rate": 2.1528150134048258e-05,
      "loss": 0.3375,
      "step": 1270
    },
    {
      "epoch": 1.8523878437047756,
      "grad_norm": 2.208824872970581,
      "learning_rate": 2.126005361930295e-05,
      "loss": 0.2788,
      "step": 1280
    },
    {
      "epoch": 1.8668596237337192,
      "grad_norm": 1.8846925497055054,
      "learning_rate": 2.0991957104557643e-05,
      "loss": 0.2881,
      "step": 1290
    },
    {
      "epoch": 1.8813314037626627,
      "grad_norm": 2.104971408843994,
      "learning_rate": 2.072386058981233e-05,
      "loss": 0.2812,
      "step": 1300
    },
    {
      "epoch": 1.8958031837916063,
      "grad_norm": 1.9642599821090698,
      "learning_rate": 2.0455764075067024e-05,
      "loss": 0.2876,
      "step": 1310
    },
    {
      "epoch": 1.9102749638205498,
      "grad_norm": 1.9819694757461548,
      "learning_rate": 2.0187667560321717e-05,
      "loss": 0.2847,
      "step": 1320
    },
    {
      "epoch": 1.9247467438494934,
      "grad_norm": 1.1249871253967285,
      "learning_rate": 1.9919571045576406e-05,
      "loss": 0.2926,
      "step": 1330
    },
    {
      "epoch": 1.939218523878437,
      "grad_norm": 1.6125729084014893,
      "learning_rate": 1.9651474530831098e-05,
      "loss": 0.3018,
      "step": 1340
    },
    {
      "epoch": 1.9536903039073805,
      "grad_norm": 2.3575940132141113,
      "learning_rate": 1.9383378016085794e-05,
      "loss": 0.3003,
      "step": 1350
    },
    {
      "epoch": 1.968162083936324,
      "grad_norm": 1.224469780921936,
      "learning_rate": 1.9115281501340483e-05,
      "loss": 0.2773,
      "step": 1360
    },
    {
      "epoch": 1.9826338639652676,
      "grad_norm": 1.3177608251571655,
      "learning_rate": 1.8847184986595175e-05,
      "loss": 0.2663,
      "step": 1370
    },
    {
      "epoch": 1.9971056439942112,
      "grad_norm": 1.7751531600952148,
      "learning_rate": 1.8579088471849868e-05,
      "loss": 0.3003,
      "step": 1380
    },
    {
      "epoch": 2.0,
      "eval_loss": 1.365147352218628,
      "eval_runtime": 86.171,
      "eval_samples_per_second": 8.031,
      "eval_steps_per_second": 4.015,
      "step": 1382
    }
  ],
  "logging_steps": 10,
  "max_steps": 2073,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2270727688290304e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
