{
  "best_metric": 0.27991819381713867,
  "best_model_checkpoint": "Qwen2.5-Coder-3B-Base-LoRA/checkpoint-2336",
  "epoch": 1.9991442019683354,
  "eval_steps": 500,
  "global_step": 2336,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.008557980316645272,
      "grad_norm": 0.21302449703216553,
      "learning_rate": 1.2820512820512822e-06,
      "loss": 1.8025,
      "step": 10
    },
    {
      "epoch": 0.017115960633290545,
      "grad_norm": 0.2375275194644928,
      "learning_rate": 2.5641025641025644e-06,
      "loss": 1.8977,
      "step": 20
    },
    {
      "epoch": 0.025673940949935817,
      "grad_norm": 0.19169671833515167,
      "learning_rate": 3.846153846153846e-06,
      "loss": 1.7464,
      "step": 30
    },
    {
      "epoch": 0.03423192126658109,
      "grad_norm": 0.3021963834762573,
      "learning_rate": 4.9999999999999996e-06,
      "loss": 2.0039,
      "step": 40
    },
    {
      "epoch": 0.04278990158322636,
      "grad_norm": 0.1804303675889969,
      "learning_rate": 6.282051282051282e-06,
      "loss": 1.9359,
      "step": 50
    },
    {
      "epoch": 0.051347881899871634,
      "grad_norm": 0.26502475142478943,
      "learning_rate": 7.564102564102564e-06,
      "loss": 1.9229,
      "step": 60
    },
    {
      "epoch": 0.0599058622165169,
      "grad_norm": 0.2673541009426117,
      "learning_rate": 8.846153846153847e-06,
      "loss": 1.8355,
      "step": 70
    },
    {
      "epoch": 0.06846384253316218,
      "grad_norm": 0.47035935521125793,
      "learning_rate": 1.012820512820513e-05,
      "loss": 1.6985,
      "step": 80
    },
    {
      "epoch": 0.07702182284980745,
      "grad_norm": 0.41896069049835205,
      "learning_rate": 1.141025641025641e-05,
      "loss": 2.0043,
      "step": 90
    },
    {
      "epoch": 0.08557980316645272,
      "grad_norm": 0.30285826325416565,
      "learning_rate": 1.2692307692307693e-05,
      "loss": 1.8962,
      "step": 100
    },
    {
      "epoch": 0.094137783483098,
      "grad_norm": 0.42871204018592834,
      "learning_rate": 1.3974358974358975e-05,
      "loss": 1.9688,
      "step": 110
    },
    {
      "epoch": 0.10269576379974327,
      "grad_norm": 0.5098448991775513,
      "learning_rate": 1.5256410256410255e-05,
      "loss": 1.7773,
      "step": 120
    },
    {
      "epoch": 0.11125374411638853,
      "grad_norm": 0.522298276424408,
      "learning_rate": 1.653846153846154e-05,
      "loss": 1.8069,
      "step": 130
    },
    {
      "epoch": 0.1198117244330338,
      "grad_norm": 0.5977389812469482,
      "learning_rate": 1.782051282051282e-05,
      "loss": 1.7071,
      "step": 140
    },
    {
      "epoch": 0.12836970474967907,
      "grad_norm": 0.640324056148529,
      "learning_rate": 1.9102564102564103e-05,
      "loss": 1.6313,
      "step": 150
    },
    {
      "epoch": 0.13692768506632436,
      "grad_norm": 1.349706768989563,
      "learning_rate": 2.0384615384615387e-05,
      "loss": 1.6416,
      "step": 160
    },
    {
      "epoch": 0.14548566538296961,
      "grad_norm": 0.9688841104507446,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 1.5029,
      "step": 170
    },
    {
      "epoch": 0.1540436456996149,
      "grad_norm": 1.610537052154541,
      "learning_rate": 2.2948717948717947e-05,
      "loss": 1.3599,
      "step": 180
    },
    {
      "epoch": 0.16260162601626016,
      "grad_norm": 1.298825979232788,
      "learning_rate": 2.423076923076923e-05,
      "loss": 1.1451,
      "step": 190
    },
    {
      "epoch": 0.17115960633290545,
      "grad_norm": 1.0291271209716797,
      "learning_rate": 2.551282051282051e-05,
      "loss": 1.0509,
      "step": 200
    },
    {
      "epoch": 0.1797175866495507,
      "grad_norm": 0.9246340394020081,
      "learning_rate": 2.67948717948718e-05,
      "loss": 0.9702,
      "step": 210
    },
    {
      "epoch": 0.188275566966196,
      "grad_norm": 1.4989973306655884,
      "learning_rate": 2.807692307692308e-05,
      "loss": 0.8803,
      "step": 220
    },
    {
      "epoch": 0.19683354728284125,
      "grad_norm": 1.0504820346832275,
      "learning_rate": 2.935897435897436e-05,
      "loss": 0.8246,
      "step": 230
    },
    {
      "epoch": 0.20539152759948653,
      "grad_norm": 1.2402162551879883,
      "learning_rate": 2.9928639391056135e-05,
      "loss": 0.8291,
      "step": 240
    },
    {
      "epoch": 0.2139495079161318,
      "grad_norm": 1.1042710542678833,
      "learning_rate": 2.978591817316841e-05,
      "loss": 0.7528,
      "step": 250
    },
    {
      "epoch": 0.22250748823277705,
      "grad_norm": 1.1832047700881958,
      "learning_rate": 2.9643196955280685e-05,
      "loss": 0.793,
      "step": 260
    },
    {
      "epoch": 0.23106546854942234,
      "grad_norm": 0.9996480345726013,
      "learning_rate": 2.950047573739296e-05,
      "loss": 0.7231,
      "step": 270
    },
    {
      "epoch": 0.2396234488660676,
      "grad_norm": 1.4231572151184082,
      "learning_rate": 2.935775451950523e-05,
      "loss": 0.5923,
      "step": 280
    },
    {
      "epoch": 0.24818142918271288,
      "grad_norm": 1.8365938663482666,
      "learning_rate": 2.9215033301617507e-05,
      "loss": 0.5628,
      "step": 290
    },
    {
      "epoch": 0.25673940949935814,
      "grad_norm": 1.1809312105178833,
      "learning_rate": 2.9072312083729782e-05,
      "loss": 0.576,
      "step": 300
    },
    {
      "epoch": 0.2652973898160034,
      "grad_norm": 1.7477322816848755,
      "learning_rate": 2.8929590865842057e-05,
      "loss": 0.5647,
      "step": 310
    },
    {
      "epoch": 0.2738553701326487,
      "grad_norm": 1.7226935625076294,
      "learning_rate": 2.8786869647954332e-05,
      "loss": 0.5844,
      "step": 320
    },
    {
      "epoch": 0.28241335044929394,
      "grad_norm": 0.9843489527702332,
      "learning_rate": 2.8644148430066604e-05,
      "loss": 0.508,
      "step": 330
    },
    {
      "epoch": 0.29097133076593923,
      "grad_norm": 5.046675682067871,
      "learning_rate": 2.851569933396765e-05,
      "loss": 0.4738,
      "step": 340
    },
    {
      "epoch": 0.2995293110825845,
      "grad_norm": 1.4436105489730835,
      "learning_rate": 2.8372978116079923e-05,
      "loss": 0.3963,
      "step": 350
    },
    {
      "epoch": 0.3080872913992298,
      "grad_norm": 1.4123443365097046,
      "learning_rate": 2.82302568981922e-05,
      "loss": 0.4565,
      "step": 360
    },
    {
      "epoch": 0.31664527171587503,
      "grad_norm": 2.57039737701416,
      "learning_rate": 2.8087535680304473e-05,
      "loss": 0.4088,
      "step": 370
    },
    {
      "epoch": 0.3252032520325203,
      "grad_norm": 3.304809093475342,
      "learning_rate": 2.7944814462416745e-05,
      "loss": 0.3869,
      "step": 380
    },
    {
      "epoch": 0.3337612323491656,
      "grad_norm": 1.410603642463684,
      "learning_rate": 2.780209324452902e-05,
      "loss": 0.4366,
      "step": 390
    },
    {
      "epoch": 0.3423192126658109,
      "grad_norm": 1.6925746202468872,
      "learning_rate": 2.7659372026641295e-05,
      "loss": 0.4082,
      "step": 400
    },
    {
      "epoch": 0.3508771929824561,
      "grad_norm": 1.3007240295410156,
      "learning_rate": 2.751665080875357e-05,
      "loss": 0.4073,
      "step": 410
    },
    {
      "epoch": 0.3594351732991014,
      "grad_norm": 1.7824610471725464,
      "learning_rate": 2.7373929590865845e-05,
      "loss": 0.4088,
      "step": 420
    },
    {
      "epoch": 0.3679931536157467,
      "grad_norm": 1.2931902408599854,
      "learning_rate": 2.7231208372978117e-05,
      "loss": 0.37,
      "step": 430
    },
    {
      "epoch": 0.376551133932392,
      "grad_norm": 1.1228828430175781,
      "learning_rate": 2.7088487155090392e-05,
      "loss": 0.3677,
      "step": 440
    },
    {
      "epoch": 0.3851091142490372,
      "grad_norm": 2.9063422679901123,
      "learning_rate": 2.6945765937202664e-05,
      "loss": 0.401,
      "step": 450
    },
    {
      "epoch": 0.3936670945656825,
      "grad_norm": 1.108642816543579,
      "learning_rate": 2.680304471931494e-05,
      "loss": 0.3785,
      "step": 460
    },
    {
      "epoch": 0.4022250748823278,
      "grad_norm": 1.2176573276519775,
      "learning_rate": 2.666032350142721e-05,
      "loss": 0.3564,
      "step": 470
    },
    {
      "epoch": 0.41078305519897307,
      "grad_norm": 3.2691798210144043,
      "learning_rate": 2.6517602283539485e-05,
      "loss": 0.3565,
      "step": 480
    },
    {
      "epoch": 0.4193410355156183,
      "grad_norm": 3.5526340007781982,
      "learning_rate": 2.637488106565176e-05,
      "loss": 0.3718,
      "step": 490
    },
    {
      "epoch": 0.4278990158322636,
      "grad_norm": 0.7667348384857178,
      "learning_rate": 2.6232159847764035e-05,
      "loss": 0.3597,
      "step": 500
    },
    {
      "epoch": 0.43645699614890887,
      "grad_norm": 2.2220826148986816,
      "learning_rate": 2.6089438629876307e-05,
      "loss": 0.3302,
      "step": 510
    },
    {
      "epoch": 0.4450149764655541,
      "grad_norm": 1.479384183883667,
      "learning_rate": 2.5946717411988582e-05,
      "loss": 0.3462,
      "step": 520
    },
    {
      "epoch": 0.4535729567821994,
      "grad_norm": 2.585080862045288,
      "learning_rate": 2.5803996194100857e-05,
      "loss": 0.3461,
      "step": 530
    },
    {
      "epoch": 0.4621309370988447,
      "grad_norm": 2.4878501892089844,
      "learning_rate": 2.5661274976213132e-05,
      "loss": 0.3292,
      "step": 540
    },
    {
      "epoch": 0.47068891741548996,
      "grad_norm": 3.7444465160369873,
      "learning_rate": 2.5518553758325407e-05,
      "loss": 0.3904,
      "step": 550
    },
    {
      "epoch": 0.4792468977321352,
      "grad_norm": 1.2132108211517334,
      "learning_rate": 2.537583254043768e-05,
      "loss": 0.3609,
      "step": 560
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 2.214503765106201,
      "learning_rate": 2.5233111322549954e-05,
      "loss": 0.3731,
      "step": 570
    },
    {
      "epoch": 0.49636285836542576,
      "grad_norm": 1.283247709274292,
      "learning_rate": 2.509039010466223e-05,
      "loss": 0.3581,
      "step": 580
    },
    {
      "epoch": 0.504920838682071,
      "grad_norm": 2.7335076332092285,
      "learning_rate": 2.49476688867745e-05,
      "loss": 0.3497,
      "step": 590
    },
    {
      "epoch": 0.5134788189987163,
      "grad_norm": 2.3100199699401855,
      "learning_rate": 2.4804947668886772e-05,
      "loss": 0.3619,
      "step": 600
    },
    {
      "epoch": 0.5220367993153616,
      "grad_norm": 0.986910343170166,
      "learning_rate": 2.4662226450999047e-05,
      "loss": 0.3512,
      "step": 610
    },
    {
      "epoch": 0.5305947796320069,
      "grad_norm": 1.4766857624053955,
      "learning_rate": 2.4519505233111322e-05,
      "loss": 0.3244,
      "step": 620
    },
    {
      "epoch": 0.5391527599486521,
      "grad_norm": 3.207575798034668,
      "learning_rate": 2.4376784015223597e-05,
      "loss": 0.3123,
      "step": 630
    },
    {
      "epoch": 0.5477107402652974,
      "grad_norm": 2.0469319820404053,
      "learning_rate": 2.423406279733587e-05,
      "loss": 0.3586,
      "step": 640
    },
    {
      "epoch": 0.5562687205819427,
      "grad_norm": 1.888210654258728,
      "learning_rate": 2.4091341579448144e-05,
      "loss": 0.31,
      "step": 650
    },
    {
      "epoch": 0.5648267008985879,
      "grad_norm": 1.2737677097320557,
      "learning_rate": 2.394862036156042e-05,
      "loss": 0.3609,
      "step": 660
    },
    {
      "epoch": 0.5733846812152332,
      "grad_norm": 1.8512996435165405,
      "learning_rate": 2.3805899143672694e-05,
      "loss": 0.3177,
      "step": 670
    },
    {
      "epoch": 0.5819426615318785,
      "grad_norm": 1.0917432308197021,
      "learning_rate": 2.366317792578497e-05,
      "loss": 0.3478,
      "step": 680
    },
    {
      "epoch": 0.5905006418485238,
      "grad_norm": 2.0401744842529297,
      "learning_rate": 2.352045670789724e-05,
      "loss": 0.342,
      "step": 690
    },
    {
      "epoch": 0.599058622165169,
      "grad_norm": 2.1338717937469482,
      "learning_rate": 2.3377735490009516e-05,
      "loss": 0.3303,
      "step": 700
    },
    {
      "epoch": 0.6076166024818143,
      "grad_norm": 2.3293874263763428,
      "learning_rate": 2.323501427212179e-05,
      "loss": 0.2634,
      "step": 710
    },
    {
      "epoch": 0.6161745827984596,
      "grad_norm": 1.9095596075057983,
      "learning_rate": 2.3092293054234066e-05,
      "loss": 0.307,
      "step": 720
    },
    {
      "epoch": 0.6247325631151048,
      "grad_norm": 1.611983299255371,
      "learning_rate": 2.2949571836346334e-05,
      "loss": 0.3328,
      "step": 730
    },
    {
      "epoch": 0.6332905434317501,
      "grad_norm": 1.149756908416748,
      "learning_rate": 2.280685061845861e-05,
      "loss": 0.2954,
      "step": 740
    },
    {
      "epoch": 0.6418485237483954,
      "grad_norm": 2.406059503555298,
      "learning_rate": 2.2664129400570884e-05,
      "loss": 0.2713,
      "step": 750
    },
    {
      "epoch": 0.6504065040650406,
      "grad_norm": 1.2950960397720337,
      "learning_rate": 2.252140818268316e-05,
      "loss": 0.2978,
      "step": 760
    },
    {
      "epoch": 0.658964484381686,
      "grad_norm": 1.6625868082046509,
      "learning_rate": 2.2378686964795435e-05,
      "loss": 0.3504,
      "step": 770
    },
    {
      "epoch": 0.6675224646983312,
      "grad_norm": 6.279384613037109,
      "learning_rate": 2.2235965746907706e-05,
      "loss": 0.2988,
      "step": 780
    },
    {
      "epoch": 0.6760804450149764,
      "grad_norm": 2.0126194953918457,
      "learning_rate": 2.209324452901998e-05,
      "loss": 0.3535,
      "step": 790
    },
    {
      "epoch": 0.6846384253316218,
      "grad_norm": 2.491755723953247,
      "learning_rate": 2.1950523311132256e-05,
      "loss": 0.3746,
      "step": 800
    },
    {
      "epoch": 0.693196405648267,
      "grad_norm": 1.4010759592056274,
      "learning_rate": 2.180780209324453e-05,
      "loss": 0.3522,
      "step": 810
    },
    {
      "epoch": 0.7017543859649122,
      "grad_norm": 1.5674467086791992,
      "learning_rate": 2.1665080875356803e-05,
      "loss": 0.3706,
      "step": 820
    },
    {
      "epoch": 0.7103123662815576,
      "grad_norm": 1.7807589769363403,
      "learning_rate": 2.1522359657469078e-05,
      "loss": 0.3242,
      "step": 830
    },
    {
      "epoch": 0.7188703465982028,
      "grad_norm": 1.8087223768234253,
      "learning_rate": 2.1379638439581353e-05,
      "loss": 0.3284,
      "step": 840
    },
    {
      "epoch": 0.727428326914848,
      "grad_norm": 2.538735866546631,
      "learning_rate": 2.1236917221693628e-05,
      "loss": 0.3379,
      "step": 850
    },
    {
      "epoch": 0.7359863072314934,
      "grad_norm": 1.4950412511825562,
      "learning_rate": 2.10941960038059e-05,
      "loss": 0.2637,
      "step": 860
    },
    {
      "epoch": 0.7445442875481386,
      "grad_norm": 2.4114863872528076,
      "learning_rate": 2.095147478591817e-05,
      "loss": 0.266,
      "step": 870
    },
    {
      "epoch": 0.753102267864784,
      "grad_norm": 2.108891010284424,
      "learning_rate": 2.0808753568030446e-05,
      "loss": 0.3127,
      "step": 880
    },
    {
      "epoch": 0.7616602481814292,
      "grad_norm": 2.501499652862549,
      "learning_rate": 2.066603235014272e-05,
      "loss": 0.2527,
      "step": 890
    },
    {
      "epoch": 0.7702182284980744,
      "grad_norm": 1.6967310905456543,
      "learning_rate": 2.0523311132254997e-05,
      "loss": 0.2925,
      "step": 900
    },
    {
      "epoch": 0.7787762088147198,
      "grad_norm": 1.8719680309295654,
      "learning_rate": 2.0380589914367268e-05,
      "loss": 0.2941,
      "step": 910
    },
    {
      "epoch": 0.787334189131365,
      "grad_norm": 2.442887306213379,
      "learning_rate": 2.0237868696479543e-05,
      "loss": 0.2923,
      "step": 920
    },
    {
      "epoch": 0.7958921694480102,
      "grad_norm": 1.9669556617736816,
      "learning_rate": 2.0095147478591818e-05,
      "loss": 0.2867,
      "step": 930
    },
    {
      "epoch": 0.8044501497646556,
      "grad_norm": 2.265255928039551,
      "learning_rate": 1.9952426260704093e-05,
      "loss": 0.3315,
      "step": 940
    },
    {
      "epoch": 0.8130081300813008,
      "grad_norm": 2.565548896789551,
      "learning_rate": 1.9809705042816365e-05,
      "loss": 0.3244,
      "step": 950
    },
    {
      "epoch": 0.8215661103979461,
      "grad_norm": 2.9796559810638428,
      "learning_rate": 1.966698382492864e-05,
      "loss": 0.3264,
      "step": 960
    },
    {
      "epoch": 0.8301240907145914,
      "grad_norm": 1.6000396013259888,
      "learning_rate": 1.9524262607040915e-05,
      "loss": 0.3118,
      "step": 970
    },
    {
      "epoch": 0.8386820710312366,
      "grad_norm": 1.5178536176681519,
      "learning_rate": 1.938154138915319e-05,
      "loss": 0.3244,
      "step": 980
    },
    {
      "epoch": 0.8472400513478819,
      "grad_norm": 2.5248091220855713,
      "learning_rate": 1.9238820171265462e-05,
      "loss": 0.3152,
      "step": 990
    },
    {
      "epoch": 0.8557980316645272,
      "grad_norm": 1.4917558431625366,
      "learning_rate": 1.9096098953377737e-05,
      "loss": 0.2929,
      "step": 1000
    },
    {
      "epoch": 0.8643560119811724,
      "grad_norm": 1.5421931743621826,
      "learning_rate": 1.895337773549001e-05,
      "loss": 0.2753,
      "step": 1010
    },
    {
      "epoch": 0.8729139922978177,
      "grad_norm": 1.908115029335022,
      "learning_rate": 1.8810656517602283e-05,
      "loss": 0.3248,
      "step": 1020
    },
    {
      "epoch": 0.881471972614463,
      "grad_norm": 1.6024490594863892,
      "learning_rate": 1.866793529971456e-05,
      "loss": 0.3115,
      "step": 1030
    },
    {
      "epoch": 0.8900299529311082,
      "grad_norm": 1.643830418586731,
      "learning_rate": 1.852521408182683e-05,
      "loss": 0.3011,
      "step": 1040
    },
    {
      "epoch": 0.8985879332477535,
      "grad_norm": 1.2896968126296997,
      "learning_rate": 1.8382492863939105e-05,
      "loss": 0.2823,
      "step": 1050
    },
    {
      "epoch": 0.9071459135643988,
      "grad_norm": 2.284728765487671,
      "learning_rate": 1.823977164605138e-05,
      "loss": 0.2844,
      "step": 1060
    },
    {
      "epoch": 0.9157038938810441,
      "grad_norm": 1.439569115638733,
      "learning_rate": 1.8097050428163655e-05,
      "loss": 0.2808,
      "step": 1070
    },
    {
      "epoch": 0.9242618741976893,
      "grad_norm": 2.3718178272247314,
      "learning_rate": 1.7954329210275927e-05,
      "loss": 0.259,
      "step": 1080
    },
    {
      "epoch": 0.9328198545143346,
      "grad_norm": 1.3777965307235718,
      "learning_rate": 1.7811607992388202e-05,
      "loss": 0.295,
      "step": 1090
    },
    {
      "epoch": 0.9413778348309799,
      "grad_norm": 1.8800636529922485,
      "learning_rate": 1.7668886774500477e-05,
      "loss": 0.3256,
      "step": 1100
    },
    {
      "epoch": 0.9499358151476252,
      "grad_norm": 1.6878808736801147,
      "learning_rate": 1.7526165556612752e-05,
      "loss": 0.2912,
      "step": 1110
    },
    {
      "epoch": 0.9584937954642704,
      "grad_norm": 1.3000328540802002,
      "learning_rate": 1.7383444338725024e-05,
      "loss": 0.3315,
      "step": 1120
    },
    {
      "epoch": 0.9670517757809157,
      "grad_norm": 2.3083364963531494,
      "learning_rate": 1.72407231208373e-05,
      "loss": 0.282,
      "step": 1130
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 1.3199355602264404,
      "learning_rate": 1.7098001902949574e-05,
      "loss": 0.3086,
      "step": 1140
    },
    {
      "epoch": 0.9841677364142063,
      "grad_norm": 2.511120080947876,
      "learning_rate": 1.6955280685061846e-05,
      "loss": 0.2987,
      "step": 1150
    },
    {
      "epoch": 0.9927257167308515,
      "grad_norm": 1.903452754020691,
      "learning_rate": 1.681255946717412e-05,
      "loss": 0.2848,
      "step": 1160
    },
    {
      "epoch": 0.9995721009841677,
      "eval_loss": 0.2985227704048157,
      "eval_runtime": 35.4538,
      "eval_samples_per_second": 16.585,
      "eval_steps_per_second": 16.585,
      "step": 1168
    },
    {
      "epoch": 1.0012836970474968,
      "grad_norm": 1.594749093055725,
      "learning_rate": 1.6669838249286392e-05,
      "loss": 0.2904,
      "step": 1170
    },
    {
      "epoch": 1.009841677364142,
      "grad_norm": 1.657151222229004,
      "learning_rate": 1.6527117031398667e-05,
      "loss": 0.2687,
      "step": 1180
    },
    {
      "epoch": 1.0183996576807874,
      "grad_norm": 2.0918354988098145,
      "learning_rate": 1.6384395813510942e-05,
      "loss": 0.2604,
      "step": 1190
    },
    {
      "epoch": 1.0269576379974326,
      "grad_norm": 1.4550237655639648,
      "learning_rate": 1.6241674595623217e-05,
      "loss": 0.3007,
      "step": 1200
    },
    {
      "epoch": 1.035515618314078,
      "grad_norm": 2.5223073959350586,
      "learning_rate": 1.609895337773549e-05,
      "loss": 0.3078,
      "step": 1210
    },
    {
      "epoch": 1.0440735986307232,
      "grad_norm": 2.609280586242676,
      "learning_rate": 1.5956232159847764e-05,
      "loss": 0.2938,
      "step": 1220
    },
    {
      "epoch": 1.0526315789473684,
      "grad_norm": 1.4072515964508057,
      "learning_rate": 1.581351094196004e-05,
      "loss": 0.2582,
      "step": 1230
    },
    {
      "epoch": 1.0611895592640137,
      "grad_norm": 1.3957659006118774,
      "learning_rate": 1.5670789724072314e-05,
      "loss": 0.321,
      "step": 1240
    },
    {
      "epoch": 1.069747539580659,
      "grad_norm": 2.6543831825256348,
      "learning_rate": 1.5528068506184586e-05,
      "loss": 0.3069,
      "step": 1250
    },
    {
      "epoch": 1.0783055198973042,
      "grad_norm": 1.496872901916504,
      "learning_rate": 1.538534728829686e-05,
      "loss": 0.2715,
      "step": 1260
    },
    {
      "epoch": 1.0868635002139495,
      "grad_norm": 1.057766318321228,
      "learning_rate": 1.5242626070409136e-05,
      "loss": 0.2957,
      "step": 1270
    },
    {
      "epoch": 1.0954214805305948,
      "grad_norm": 1.5130765438079834,
      "learning_rate": 1.5099904852521411e-05,
      "loss": 0.2879,
      "step": 1280
    },
    {
      "epoch": 1.10397946084724,
      "grad_norm": 2.722623586654663,
      "learning_rate": 1.4957183634633683e-05,
      "loss": 0.2997,
      "step": 1290
    },
    {
      "epoch": 1.1125374411638853,
      "grad_norm": 1.7959027290344238,
      "learning_rate": 1.4814462416745958e-05,
      "loss": 0.2886,
      "step": 1300
    },
    {
      "epoch": 1.1210954214805307,
      "grad_norm": 1.8162052631378174,
      "learning_rate": 1.4671741198858231e-05,
      "loss": 0.2918,
      "step": 1310
    },
    {
      "epoch": 1.1296534017971758,
      "grad_norm": 2.4241578578948975,
      "learning_rate": 1.4543292102759277e-05,
      "loss": 0.3438,
      "step": 1320
    },
    {
      "epoch": 1.1382113821138211,
      "grad_norm": 1.3565338850021362,
      "learning_rate": 1.440057088487155e-05,
      "loss": 0.3341,
      "step": 1330
    },
    {
      "epoch": 1.1467693624304665,
      "grad_norm": 2.510615348815918,
      "learning_rate": 1.4257849666983824e-05,
      "loss": 0.2856,
      "step": 1340
    },
    {
      "epoch": 1.1553273427471118,
      "grad_norm": 1.2238510847091675,
      "learning_rate": 1.41151284490961e-05,
      "loss": 0.3009,
      "step": 1350
    },
    {
      "epoch": 1.163885323063757,
      "grad_norm": 2.529473304748535,
      "learning_rate": 1.3972407231208373e-05,
      "loss": 0.2769,
      "step": 1360
    },
    {
      "epoch": 1.1724433033804023,
      "grad_norm": 0.9646504521369934,
      "learning_rate": 1.3829686013320648e-05,
      "loss": 0.2632,
      "step": 1370
    },
    {
      "epoch": 1.1810012836970474,
      "grad_norm": 1.422224521636963,
      "learning_rate": 1.3686964795432923e-05,
      "loss": 0.3044,
      "step": 1380
    },
    {
      "epoch": 1.1895592640136927,
      "grad_norm": 1.186933994293213,
      "learning_rate": 1.3544243577545196e-05,
      "loss": 0.2468,
      "step": 1390
    },
    {
      "epoch": 1.198117244330338,
      "grad_norm": 1.7655137777328491,
      "learning_rate": 1.340152235965747e-05,
      "loss": 0.2814,
      "step": 1400
    },
    {
      "epoch": 1.2066752246469834,
      "grad_norm": 1.0088876485824585,
      "learning_rate": 1.3258801141769743e-05,
      "loss": 0.2794,
      "step": 1410
    },
    {
      "epoch": 1.2152332049636285,
      "grad_norm": 1.161863923072815,
      "learning_rate": 1.3116079923882018e-05,
      "loss": 0.2519,
      "step": 1420
    },
    {
      "epoch": 1.2237911852802739,
      "grad_norm": 1.7435027360916138,
      "learning_rate": 1.2973358705994291e-05,
      "loss": 0.2795,
      "step": 1430
    },
    {
      "epoch": 1.2323491655969192,
      "grad_norm": 1.8468748331069946,
      "learning_rate": 1.2830637488106566e-05,
      "loss": 0.2631,
      "step": 1440
    },
    {
      "epoch": 1.2409071459135643,
      "grad_norm": 2.3253822326660156,
      "learning_rate": 1.268791627021884e-05,
      "loss": 0.2874,
      "step": 1450
    },
    {
      "epoch": 1.2494651262302097,
      "grad_norm": 1.130916714668274,
      "learning_rate": 1.2545195052331115e-05,
      "loss": 0.2704,
      "step": 1460
    },
    {
      "epoch": 1.258023106546855,
      "grad_norm": 1.608939290046692,
      "learning_rate": 1.2402473834443386e-05,
      "loss": 0.2645,
      "step": 1470
    },
    {
      "epoch": 1.2665810868635003,
      "grad_norm": 1.4493552446365356,
      "learning_rate": 1.2259752616555661e-05,
      "loss": 0.3151,
      "step": 1480
    },
    {
      "epoch": 1.2751390671801455,
      "grad_norm": 2.2563209533691406,
      "learning_rate": 1.2117031398667935e-05,
      "loss": 0.3174,
      "step": 1490
    },
    {
      "epoch": 1.2836970474967908,
      "grad_norm": 1.8586705923080444,
      "learning_rate": 1.197431018078021e-05,
      "loss": 0.3147,
      "step": 1500
    },
    {
      "epoch": 1.292255027813436,
      "grad_norm": 1.55685293674469,
      "learning_rate": 1.1831588962892485e-05,
      "loss": 0.2964,
      "step": 1510
    },
    {
      "epoch": 1.3008130081300813,
      "grad_norm": 2.1065011024475098,
      "learning_rate": 1.1688867745004758e-05,
      "loss": 0.2347,
      "step": 1520
    },
    {
      "epoch": 1.3093709884467266,
      "grad_norm": 2.0191092491149902,
      "learning_rate": 1.1546146527117033e-05,
      "loss": 0.2864,
      "step": 1530
    },
    {
      "epoch": 1.317928968763372,
      "grad_norm": 2.215963125228882,
      "learning_rate": 1.1403425309229305e-05,
      "loss": 0.2858,
      "step": 1540
    },
    {
      "epoch": 1.326486949080017,
      "grad_norm": 1.5965423583984375,
      "learning_rate": 1.126070409134158e-05,
      "loss": 0.2818,
      "step": 1550
    },
    {
      "epoch": 1.3350449293966624,
      "grad_norm": 1.5914149284362793,
      "learning_rate": 1.1117982873453853e-05,
      "loss": 0.2791,
      "step": 1560
    },
    {
      "epoch": 1.3436029097133075,
      "grad_norm": 2.0537281036376953,
      "learning_rate": 1.0975261655566128e-05,
      "loss": 0.2739,
      "step": 1570
    },
    {
      "epoch": 1.3521608900299529,
      "grad_norm": 1.4896125793457031,
      "learning_rate": 1.0832540437678401e-05,
      "loss": 0.2737,
      "step": 1580
    },
    {
      "epoch": 1.3607188703465982,
      "grad_norm": 2.3912792205810547,
      "learning_rate": 1.0689819219790677e-05,
      "loss": 0.2686,
      "step": 1590
    },
    {
      "epoch": 1.3692768506632436,
      "grad_norm": 2.891601324081421,
      "learning_rate": 1.054709800190295e-05,
      "loss": 0.2795,
      "step": 1600
    },
    {
      "epoch": 1.3778348309798887,
      "grad_norm": 2.594637393951416,
      "learning_rate": 1.0404376784015223e-05,
      "loss": 0.2753,
      "step": 1610
    },
    {
      "epoch": 1.386392811296534,
      "grad_norm": 1.8519787788391113,
      "learning_rate": 1.0261655566127498e-05,
      "loss": 0.3011,
      "step": 1620
    },
    {
      "epoch": 1.3949507916131794,
      "grad_norm": 1.9440478086471558,
      "learning_rate": 1.0118934348239772e-05,
      "loss": 0.2657,
      "step": 1630
    },
    {
      "epoch": 1.4035087719298245,
      "grad_norm": 1.2459481954574585,
      "learning_rate": 9.976213130352047e-06,
      "loss": 0.2677,
      "step": 1640
    },
    {
      "epoch": 1.4120667522464698,
      "grad_norm": 1.1933913230895996,
      "learning_rate": 9.83349191246432e-06,
      "loss": 0.303,
      "step": 1650
    },
    {
      "epoch": 1.4206247325631152,
      "grad_norm": 1.0470231771469116,
      "learning_rate": 9.690770694576595e-06,
      "loss": 0.2493,
      "step": 1660
    },
    {
      "epoch": 1.4291827128797605,
      "grad_norm": 1.5036952495574951,
      "learning_rate": 9.548049476688868e-06,
      "loss": 0.2531,
      "step": 1670
    },
    {
      "epoch": 1.4377406931964056,
      "grad_norm": 1.7584211826324463,
      "learning_rate": 9.405328258801142e-06,
      "loss": 0.2815,
      "step": 1680
    },
    {
      "epoch": 1.446298673513051,
      "grad_norm": 1.6069337129592896,
      "learning_rate": 9.262607040913415e-06,
      "loss": 0.3057,
      "step": 1690
    },
    {
      "epoch": 1.454856653829696,
      "grad_norm": 1.8797067403793335,
      "learning_rate": 9.11988582302569e-06,
      "loss": 0.2516,
      "step": 1700
    },
    {
      "epoch": 1.4634146341463414,
      "grad_norm": 2.387932300567627,
      "learning_rate": 8.977164605137963e-06,
      "loss": 0.3131,
      "step": 1710
    },
    {
      "epoch": 1.4719726144629868,
      "grad_norm": 2.6823039054870605,
      "learning_rate": 8.834443387250239e-06,
      "loss": 0.3073,
      "step": 1720
    },
    {
      "epoch": 1.4805305947796321,
      "grad_norm": 1.6844232082366943,
      "learning_rate": 8.691722169362512e-06,
      "loss": 0.2603,
      "step": 1730
    },
    {
      "epoch": 1.4890885750962772,
      "grad_norm": 1.0911082029342651,
      "learning_rate": 8.549000951474787e-06,
      "loss": 0.3101,
      "step": 1740
    },
    {
      "epoch": 1.4976465554129226,
      "grad_norm": 1.3146289587020874,
      "learning_rate": 8.40627973358706e-06,
      "loss": 0.2501,
      "step": 1750
    },
    {
      "epoch": 1.5062045357295677,
      "grad_norm": 1.8416701555252075,
      "learning_rate": 8.263558515699334e-06,
      "loss": 0.2858,
      "step": 1760
    },
    {
      "epoch": 1.514762516046213,
      "grad_norm": 1.8846379518508911,
      "learning_rate": 8.120837297811609e-06,
      "loss": 0.3051,
      "step": 1770
    },
    {
      "epoch": 1.5233204963628584,
      "grad_norm": 2.5063745975494385,
      "learning_rate": 7.978116079923882e-06,
      "loss": 0.262,
      "step": 1780
    },
    {
      "epoch": 1.5318784766795037,
      "grad_norm": 1.3514279127120972,
      "learning_rate": 7.835394862036157e-06,
      "loss": 0.2639,
      "step": 1790
    },
    {
      "epoch": 1.540436456996149,
      "grad_norm": 2.135018825531006,
      "learning_rate": 7.69267364414843e-06,
      "loss": 0.2461,
      "step": 1800
    },
    {
      "epoch": 1.5489944373127942,
      "grad_norm": 1.8335882425308228,
      "learning_rate": 7.5499524262607054e-06,
      "loss": 0.278,
      "step": 1810
    },
    {
      "epoch": 1.5575524176294393,
      "grad_norm": 1.3648045063018799,
      "learning_rate": 7.407231208372979e-06,
      "loss": 0.2077,
      "step": 1820
    },
    {
      "epoch": 1.5661103979460846,
      "grad_norm": 2.1583526134490967,
      "learning_rate": 7.264509990485253e-06,
      "loss": 0.2924,
      "step": 1830
    },
    {
      "epoch": 1.57466837826273,
      "grad_norm": 2.1809332370758057,
      "learning_rate": 7.121788772597526e-06,
      "loss": 0.2737,
      "step": 1840
    },
    {
      "epoch": 1.5832263585793753,
      "grad_norm": 2.521221399307251,
      "learning_rate": 6.9790675547098005e-06,
      "loss": 0.2788,
      "step": 1850
    },
    {
      "epoch": 1.5917843388960207,
      "grad_norm": 1.3835545778274536,
      "learning_rate": 6.836346336822075e-06,
      "loss": 0.2911,
      "step": 1860
    },
    {
      "epoch": 1.6003423192126658,
      "grad_norm": 3.5679516792297363,
      "learning_rate": 6.693625118934348e-06,
      "loss": 0.3141,
      "step": 1870
    },
    {
      "epoch": 1.6089002995293111,
      "grad_norm": 1.3276774883270264,
      "learning_rate": 6.550903901046622e-06,
      "loss": 0.2586,
      "step": 1880
    },
    {
      "epoch": 1.6174582798459562,
      "grad_norm": 1.3102561235427856,
      "learning_rate": 6.4081826831588965e-06,
      "loss": 0.2952,
      "step": 1890
    },
    {
      "epoch": 1.6260162601626016,
      "grad_norm": 1.2468198537826538,
      "learning_rate": 6.265461465271171e-06,
      "loss": 0.279,
      "step": 1900
    },
    {
      "epoch": 1.634574240479247,
      "grad_norm": 1.5694257020950317,
      "learning_rate": 6.122740247383444e-06,
      "loss": 0.3165,
      "step": 1910
    },
    {
      "epoch": 1.6431322207958923,
      "grad_norm": 1.5558409690856934,
      "learning_rate": 5.980019029495718e-06,
      "loss": 0.2574,
      "step": 1920
    },
    {
      "epoch": 1.6516902011125374,
      "grad_norm": 2.0969672203063965,
      "learning_rate": 5.837297811607992e-06,
      "loss": 0.2992,
      "step": 1930
    },
    {
      "epoch": 1.6602481814291827,
      "grad_norm": 2.102635145187378,
      "learning_rate": 5.694576593720267e-06,
      "loss": 0.2773,
      "step": 1940
    },
    {
      "epoch": 1.6688061617458279,
      "grad_norm": 0.8721839785575867,
      "learning_rate": 5.551855375832541e-06,
      "loss": 0.2914,
      "step": 1950
    },
    {
      "epoch": 1.6773641420624732,
      "grad_norm": 1.8938199281692505,
      "learning_rate": 5.409134157944815e-06,
      "loss": 0.3395,
      "step": 1960
    },
    {
      "epoch": 1.6859221223791185,
      "grad_norm": 1.132170557975769,
      "learning_rate": 5.266412940057089e-06,
      "loss": 0.3011,
      "step": 1970
    },
    {
      "epoch": 1.6944801026957639,
      "grad_norm": 1.5981011390686035,
      "learning_rate": 5.1236917221693625e-06,
      "loss": 0.2675,
      "step": 1980
    },
    {
      "epoch": 1.7030380830124092,
      "grad_norm": 2.099080801010132,
      "learning_rate": 4.980970504281637e-06,
      "loss": 0.2572,
      "step": 1990
    },
    {
      "epoch": 1.7115960633290543,
      "grad_norm": 1.706209659576416,
      "learning_rate": 4.838249286393911e-06,
      "loss": 0.2549,
      "step": 2000
    },
    {
      "epoch": 1.7201540436456995,
      "grad_norm": 1.7829123735427856,
      "learning_rate": 4.695528068506184e-06,
      "loss": 0.2959,
      "step": 2010
    },
    {
      "epoch": 1.7287120239623448,
      "grad_norm": 2.4995434284210205,
      "learning_rate": 4.5528068506184585e-06,
      "loss": 0.3404,
      "step": 2020
    },
    {
      "epoch": 1.7372700042789901,
      "grad_norm": 1.486979603767395,
      "learning_rate": 4.410085632730733e-06,
      "loss": 0.243,
      "step": 2030
    },
    {
      "epoch": 1.7458279845956355,
      "grad_norm": 1.7111248970031738,
      "learning_rate": 4.267364414843007e-06,
      "loss": 0.3196,
      "step": 2040
    },
    {
      "epoch": 1.7543859649122808,
      "grad_norm": 1.4619425535202026,
      "learning_rate": 4.12464319695528e-06,
      "loss": 0.2737,
      "step": 2050
    },
    {
      "epoch": 1.762943945228926,
      "grad_norm": 3.018691301345825,
      "learning_rate": 3.981921979067554e-06,
      "loss": 0.2458,
      "step": 2060
    },
    {
      "epoch": 1.7715019255455713,
      "grad_norm": 1.7187697887420654,
      "learning_rate": 3.8392007611798295e-06,
      "loss": 0.2769,
      "step": 2070
    },
    {
      "epoch": 1.7800599058622164,
      "grad_norm": 1.5385628938674927,
      "learning_rate": 3.696479543292103e-06,
      "loss": 0.3146,
      "step": 2080
    },
    {
      "epoch": 1.7886178861788617,
      "grad_norm": 3.0653417110443115,
      "learning_rate": 3.5537583254043766e-06,
      "loss": 0.2774,
      "step": 2090
    },
    {
      "epoch": 1.797175866495507,
      "grad_norm": 1.5907059907913208,
      "learning_rate": 3.411037107516651e-06,
      "loss": 0.3216,
      "step": 2100
    },
    {
      "epoch": 1.8057338468121524,
      "grad_norm": 1.3927608728408813,
      "learning_rate": 3.268315889628925e-06,
      "loss": 0.2965,
      "step": 2110
    },
    {
      "epoch": 1.8142918271287976,
      "grad_norm": 1.5576879978179932,
      "learning_rate": 3.125594671741199e-06,
      "loss": 0.227,
      "step": 2120
    },
    {
      "epoch": 1.822849807445443,
      "grad_norm": 1.9193435907363892,
      "learning_rate": 2.982873453853473e-06,
      "loss": 0.2745,
      "step": 2130
    },
    {
      "epoch": 1.831407787762088,
      "grad_norm": 1.201753854751587,
      "learning_rate": 2.8401522359657467e-06,
      "loss": 0.2526,
      "step": 2140
    },
    {
      "epoch": 1.8399657680787334,
      "grad_norm": 1.3830738067626953,
      "learning_rate": 2.697431018078021e-06,
      "loss": 0.3117,
      "step": 2150
    },
    {
      "epoch": 1.8485237483953787,
      "grad_norm": 2.9717888832092285,
      "learning_rate": 2.554709800190295e-06,
      "loss": 0.2809,
      "step": 2160
    },
    {
      "epoch": 1.857081728712024,
      "grad_norm": 1.3902703523635864,
      "learning_rate": 2.4119885823025693e-06,
      "loss": 0.248,
      "step": 2170
    },
    {
      "epoch": 1.8656397090286694,
      "grad_norm": 1.6077473163604736,
      "learning_rate": 2.269267364414843e-06,
      "loss": 0.3152,
      "step": 2180
    },
    {
      "epoch": 1.8741976893453145,
      "grad_norm": 1.5366302728652954,
      "learning_rate": 2.1265461465271173e-06,
      "loss": 0.2862,
      "step": 2190
    },
    {
      "epoch": 1.8827556696619596,
      "grad_norm": 1.739312767982483,
      "learning_rate": 1.983824928639391e-06,
      "loss": 0.2542,
      "step": 2200
    },
    {
      "epoch": 1.891313649978605,
      "grad_norm": 1.8554072380065918,
      "learning_rate": 1.8411037107516652e-06,
      "loss": 0.2753,
      "step": 2210
    },
    {
      "epoch": 1.8998716302952503,
      "grad_norm": 1.5925499200820923,
      "learning_rate": 1.6983824928639392e-06,
      "loss": 0.273,
      "step": 2220
    },
    {
      "epoch": 1.9084296106118956,
      "grad_norm": 2.6086301803588867,
      "learning_rate": 1.555661274976213e-06,
      "loss": 0.2528,
      "step": 2230
    },
    {
      "epoch": 1.916987590928541,
      "grad_norm": 1.993685245513916,
      "learning_rate": 1.4129400570884872e-06,
      "loss": 0.2628,
      "step": 2240
    },
    {
      "epoch": 1.925545571245186,
      "grad_norm": 1.2155303955078125,
      "learning_rate": 1.2702188392007612e-06,
      "loss": 0.2383,
      "step": 2250
    },
    {
      "epoch": 1.9341035515618314,
      "grad_norm": 2.118440866470337,
      "learning_rate": 1.1274976213130352e-06,
      "loss": 0.3193,
      "step": 2260
    },
    {
      "epoch": 1.9426615318784766,
      "grad_norm": 1.5611988306045532,
      "learning_rate": 9.847764034253094e-07,
      "loss": 0.2697,
      "step": 2270
    },
    {
      "epoch": 1.951219512195122,
      "grad_norm": 1.5360392332077026,
      "learning_rate": 8.420551855375832e-07,
      "loss": 0.2797,
      "step": 2280
    },
    {
      "epoch": 1.9597774925117672,
      "grad_norm": 1.3810043334960938,
      "learning_rate": 6.993339676498573e-07,
      "loss": 0.2796,
      "step": 2290
    },
    {
      "epoch": 1.9683354728284126,
      "grad_norm": 1.455923318862915,
      "learning_rate": 5.566127497621313e-07,
      "loss": 0.2784,
      "step": 2300
    },
    {
      "epoch": 1.976893453145058,
      "grad_norm": 1.2680728435516357,
      "learning_rate": 4.1389153187440535e-07,
      "loss": 0.2873,
      "step": 2310
    },
    {
      "epoch": 1.985451433461703,
      "grad_norm": 3.029378652572632,
      "learning_rate": 2.711703139866794e-07,
      "loss": 0.2817,
      "step": 2320
    },
    {
      "epoch": 1.9940094137783482,
      "grad_norm": 1.5499979257583618,
      "learning_rate": 1.2844909609895337e-07,
      "loss": 0.2838,
      "step": 2330
    },
    {
      "epoch": 1.9991442019683354,
      "eval_loss": 0.27991819381713867,
      "eval_runtime": 36.0817,
      "eval_samples_per_second": 16.296,
      "eval_steps_per_second": 16.296,
      "step": 2336
    }
  ],
  "logging_steps": 10,
  "max_steps": 2336,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.985107283083264e+16,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
